{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三章：编码注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本notebook会用到以下第三方库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.1+cu118\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 本章节覆盖LLMs的引擎--注意力机制\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/01.webp?123\" width=\"500px\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/02.webp\" width=\"600px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 建模长序列的问题\n",
    "- 本小结没有代码\n",
    "- 在不同语种文本翻译任务中，因为不同语言的语法结构不同导致逐词翻译是不可行的\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/03.webp\" width=\"400px\">\n",
    "</p>\n",
    "\n",
    "- 在transformer模型出现之前，编码器-解码器RNN通常用于机器翻译任务\n",
    "- 在这种设置中，编码器处理源语言的标记序列，使用隐藏状态（神经网络内部的一种中间层）来生成整个输入序列的浓缩表示\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/04.webp\" width=\"600px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 使用注意力机制捕捉数据之间的依赖\n",
    "- 本小结没有代码\n",
    "- 通过注意力机制，网络的文本生成解码器部分能够选择性地访问所有输入标记，这意味着在生成特定输出标记时，某些输入标记比其他标记更重要\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/05.webp\" width=\"600px\">\n",
    "</p>\n",
    "\n",
    "- 自注意力在transformer中是一种旨在增强输入表示的技术，它使序列中的每个位置都能与同一序列中的所有其他位置进行交互并确定它们的相关性\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/06.webp\" width=\"300px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 通过自注意力关注输入的不同部分\n",
    "\n",
    "### 3.3.1 一个不带可训练权重的简单自注意力机制\n",
    "- 本节解释了一个非常简化的自注意力变体，它不包含任何可训练的权重\n",
    "- 这纯粹是为了说明目的，而不是transformer中使用的注意力机制\n",
    "- 下一节，3.3.2节，将扩展这个简单的注意力机制来实现真正的自注意力机制\n",
    "- 假设我们有一个输入序列$x^{(1)}$到$x^{(T)}$\n",
    "  - 输入是一段文本（例如，一个句子如\"Your journey starts with one step\"），已经按照第2章所述转换为标记嵌入\n",
    "  - 例如，$x^{(1)}$是表示单词\"Your\"的d维向量，以此类推\n",
    "- **目标：**为输入序列$x^{(1)}$到$x^{(T)}$中的每个输入序列元素$x^{(i)}$计算上下文向量$z^{(i)}$（其中$z$和$x$具有相同的维度）\n",
    "    - 上下文向量$z^{(i)}$是输入$x^{(1)}$到$x^{(T)}$的加权和\n",
    "    - 上下文向量对特定输入是\"上下文\"特定的\n",
    "      - 不用$x^{(i)}$作为任意输入标记的占位符，关于第二个输入$x^{(2)}$\n",
    "      - 继续使用具体示例，不用占位符$z^{(i)}$，考虑第二个输出上下文向量$z^{(2)}$\n",
    "      - 第二个上下文向量$z^{(2)}$是所有输入$x^{(1)}$到$x^{(T)}$的加权和，权重是相对于第二个输入元素$x^{(2)}$的\n",
    "      - 注意力权重是决定在计算$z^{(2)}$时每个输入元素贡献多少的权重\n",
    "      - 简而言之，可以将$z^{(2)}$视为$x^{(2)}$的修改版本，它还包含了与给定任务相关的所有其他输入元素的信息\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/07.webp\" width=\"400px\">\n",
    "</p>\n",
    "\n",
    "- （请注意，此图中的数字截断为小数点后一位，以减少视觉混乱；类似地，其他图表也可能包含截断的值）\n",
    "\n",
    "- 按照惯例，未归一化的注意力权重被称为\"注意力分数\"，而归一化后和为1的注意力分数则被称为\"注意力权重\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下面的代码逐步解释了上图中的过程\n",
    "\n",
    "<br>\n",
    "\n",
    "- 第一步：计算未归一化的注意力分数 $\\omega$\n",
    "- 假设使用第二个input token作为query，即 $q^{(2)} = x^{(2)}$，通过以下点乘操作计算为归一化分数\n",
    "    - $\\omega_{21} = x^{(1)} q^{(2)\\top}$\n",
    "    - $\\omega_{22} = x^{(2)} q^{(2)\\top}$\n",
    "    - $\\omega_{23} = x^{(3)} q^{(2)\\top}$\n",
    "    - ...\n",
    "    - $\\omega_{2T} = x^{(T)} q^{(2)\\top}$\n",
    "- 上面的 $\\omega$ 是希腊字母\"omega\"，用来表示未归一化的注意力分数\n",
    "    - 下标\"21\"在$\\omega_{21}$中表示输入序列元素2被用作针对输入序列元素1的查询\n",
    "\n",
    "- 假设有以下输入句子，已经按照第3章所述嵌入到3维向量中（这里为了说明目的使用非常小的嵌入维度，这样它可以在页面上不换行地显示）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- （在本书中遵循常见的机器学习和深度学习惯例，其中训练样本表示为行，特征值表示为列；在上面显示的张量中，每一行代表一个词，每一列代表一个嵌入维度）\n",
    "\n",
    "- 本节的主要目标是演示如何使用第二个输入序列$x^{(2)}$作为查询来计算上下文向量$z^{(2)}$\n",
    "\n",
    "- 图中描述了这个过程的初始步骤，即通过点积操作计算$x^{(2)}$与所有其他输入元素之间的注意力分数ω\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/08.webp\" width=\"400px\">\n",
    "</p>\n",
    "\n",
    "- 使用输入序列元素2，$x^{(2)}$，作为示例来计算上下文向量$z^{(2)}$；在本节后面，将推广这一点来计算所有上下文向量。\n",
    "\n",
    "- 第一步是通过计算查询$x^{(2)}$与所有其他输入标记之间的点积来计算未归一化的注意力分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 附注：点积本质上是将两个向量按元素相乘并对结果求和的简写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "res = 0.\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **步骤2**：归一化未归一化的注意力分数（\"omega\"，$\\omega$），使它们的总和为1\n",
    "- 这里是一种将未归一化注意力分数归一化为总和为1的简单方法（这是一种惯例，有助于解释，并且对训练稳定性很重要）\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/09.webp\" width=\"500px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 然而，在实践中，使用softmax函数进行归一化是常见且推荐的，它更善于处理极端值，并且在训练期间具有更理想的梯度特性\n",
    "- 这里是一个用于缩放的softmax函数的简单实现，它也将向量元素归一化，使它们的总和为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 上面的简单实现可能会因为大或小的输入值而遇到数值不稳定问题，这是由于溢出和下溢问题\n",
    "- 因此，在实践中，建议使用PyTorch的softmax实现，它已经为性能进行了高度优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **步骤3**：计算上下文向量$z^{(2)}$，方法是将嵌入的输入标记$x^{(i)}$与注意力权重相乘，然后对结果向量求和\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/10.webp\" width=\"500px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 对所有输入tokens计算注意力权重\n",
    "\n",
    "#### 概括所有输入序列tokens\n",
    "\n",
    "- 上面已计算了输入2的注意力权重和上下文向量（如下图中突出显示的行所示）\n",
    "- 接下来，将这种计算推广到计算所有注意力权重和上下文向量\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/11.webp\" width=\"400px\">\n",
    "</p>\n",
    "\n",
    "- （请注意，此图中的数字截断为小数点后两位，以减少视觉混乱；每行中的值应加起来等于1.0或100%；类似地，其他图中的数字也被截断）\n",
    "\n",
    "- 自注意力过程从计算注意力分数开始，然后将其归一化以得到总和为1的注意力权重\n",
    "- 这些注意力权重随后通过输入的加权求和来生成上下文向量\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/12.webp\" width=\"400px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将之前的**步骤1**应用于所有成对元素，计算未归一化的注意力分数矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以通过矩阵乘法更高效地实现与上述相同的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 与之前的**步骤2**类似，对每一行进行归一化，使每行中的值总和为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 快速验证每行中的值确实总和为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 应用之前的**步骤3**来计算所有上下文向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 作为合理性检查，之前计算的上下文向量$z^{(2)} = [0.4419, 0.6515, 0.5683]$可以在上面的第2行找到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
