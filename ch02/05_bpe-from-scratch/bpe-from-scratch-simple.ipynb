{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dec0dfb-3d60-41d0-a63a-b010dce67e32",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "由 <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a> 撰写的 <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> 一书的补充代码<br>\n",
    "<br>代码仓库：<a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e475425-8300-43f2-a5e8-6b5d2de59925",
   "metadata": {},
   "source": [
    "# 从零实现字节对编码 (BPE) 分词器——简化版"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfc3f3-8ec1-4fd3-b378-d9a3d7807a54",
   "metadata": {},
   "source": [
    "- 这是一个独立的笔记本，出于教学目的从零实现了流行的字节对编码（BPE）分词算法，该算法被GPT-2至GPT-4、Llama 3等模型采用\n",
    "- 关于分词用途的更多细节，请参考[第 2 章](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)；这个代码部分是用于解释BPE算法的额外内容\n",
    "- OpenAI用于训练早期GPT模型的原始BPE分词器可以在[这里](https://github.com/openai/gpt-2/blob/master/src/encoder.py)找到\n",
    "- BPE算法最初由Philip Gage在1994年的\"[A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\"中描述\n",
    "- 由于计算性能优势，大多数项目（包括Llama 3）如今使用OpenAI开源的[tiktoken 库](https://github.com/openai/tiktoken)；它可以加载预训练的GPT-2与GPT-4分词器（Llama 3模型也使用GPT-4分词器进行训练）\n",
    "- 上述实现与本笔记本中的实现的不同之处在于：这里额外提供一个用于训练分词器的函数（仅限于教学目的）\n",
    "- 还有一个名为[minBPE](https://github.com/karpathy/minbpe)的实现，支持训练，可能更加高效（此处的实现更注重教学）；与`minbpe`相比，本notebook实现还可以加载OpenAI的原始分词器词表和合并规则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910acd61-8947-4cfa-962f-16f4c733f2db",
   "metadata": {},
   "source": [
    "**这是一个非常直白的教学实现。[bpe-from-scratch.ipynb](bpe-from-scratch.ipynb)中提供了一个更复杂（但也更难读）的实现，其行为与 tiktoken 保持一致。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62336db-f45c-4894-9167-7583095dbdf1",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 1. 字节对编码 (BPE) 的核心思想"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f1231-bd42-41b5-a017-974b8c660a44",
   "metadata": {},
   "source": [
    "- BPE的核心思想是将文本转换成整数表示（token ID），以便训练大语言模型（参见[第 2 章](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)）\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/bpe-overview.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c625d-26a1-4896-98a2-0fdcd1591256",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.1 位与字节"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddaa35-0ed7-4012-827e-911de11c266c",
   "metadata": {},
   "source": [
    "- 在就BPE算法本身之前，先介绍一下字节的概念\n",
    "- 思考将文本转换为一个字节数组（正如BPE是\"byte pair encoding\"的缩写）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c9bc9e4-120f-4bac-8fa6-6523c568d12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text')\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd92a2a-9d74-4dc7-bb53-ac33d6cf2fab",
   "metadata": {},
   "source": [
    "- 当对`bytearray`对象调用`list()`时，每个字节都会被视为独立元素，从而得到一个由这些字节值构成的整数列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c586945-d459-4f9a-855d-bf73438ef0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_ary)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efea37-f4c3-4cb8-bfa5-9299175faf9a",
   "metadata": {},
   "source": [
    "- 这确实是一种可以将文本转换为token ID表示的模式，可供LLM的嵌入层使用\n",
    "- 但是，这种做法的缺点是需要为每个字符创建一个ID（对于短文本也会产生大量 ID）\n",
    "- 也就是说，对于一段17个字符的输入文本，会向LLM提供17个token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5b61d9-79a0-48b4-9b3e-64ab595c5b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 17\n",
      "Number of token IDs: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc833a-c0d4-4d46-9180-c0042fd6addc",
   "metadata": {},
   "source": [
    "- 如果使用过LLM，就会知道BPE分词器的词表中，为整个单词或子词分配token ID，而不是每个单个字符\n",
    "- 例如，GPT-2分词器会将相同的文本（\"This is some text\"）分成4个token，而非17个：`1212, 318, 617, 2420`\n",
    "- 可以使用交互式[tiktoken app](https://tiktokenizer.vercel.app/?model=gpt2)或[tiktoken 库](https://github.com/openai/tiktoken)来三重验证：\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/tiktokenizer.webp\" width=\"600px\">\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")\n",
    "# prints [1212, 318, 617, 2420]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b99de-cbfc-441c-8b3e-296a5dd7bb27",
   "metadata": {},
   "source": [
    "- 由于一个字节由 8 个位组成，因此一个字节可表示 2<sup>8</sup> = 256 个可能值，范围为 0 至 255\n",
    "- 可以通过执行`bytearray(range(0, 257))`来验证，会报告`ValueError: byte must be in range(0, 256)`\n",
    "- BPE分词器通常使用这256个值作为第一批单字符token；可以通过下面的代码直接看到\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")\n",
    "\"\"\"\n",
    "prints:\n",
    "0: !\n",
    "1: \"\n",
    "2: #\n",
    "...\n",
    "255: �? # <---- single character tokens up to here\n",
    "256:  t\n",
    "257:  a\n",
    "...\n",
    "298: ent\n",
    "299:  n\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff0207-7f8e-44fa-9381-2a4bd83daab3",
   "metadata": {},
   "source": [
    "- 上面的列表中，第256和257项并非单字符值，而是两个字符（空格 + 字母），这是原始GPT-2 BPE分词器的一个小缺点（在GPT-4的分词器中已得到改进）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241c23a-d487-488d-bded-cdf054e24920",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.2 构建词表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2ceb7-0b3f-4a62-8dcc-07810cd8886e",
   "metadata": {},
   "source": [
    "- BPE分词算法的目标是构建一个由常见子词构成的词表，比如`298: ent`（例如 *entangle, entertain, enter, entrance, entity, ...* 中都可以找到），或者包含完整单词，例如\n",
    "\n",
    "```\n",
    "318: is\n",
    "617: some\n",
    "1212: This\n",
    "2420: text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d4420-a4c7-4813-916a-06f4f46bc3f0",
   "metadata": {},
   "source": [
    "- BPE算法最初由Philip Gage在1994年的\"[A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\"中描述\n",
    "- 在深入代码实现之前，现代 LLM 分词器使用的形式可以概括如下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc71db9-b070-48c4-8412-81f45b308ab3",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.3 BPE算法概览\n",
    "\n",
    "**1. 识别常见字节对**\n",
    "- 在每次迭代中，扫描文本，找到最常出现的两个接连字节（或字符）\n",
    "\n",
    "**2. 替换并记录**\n",
    "\n",
    "- 将这对字节用一个新ID代替（不能和现有ID冲突，例如从0...255开始时，第一个新ID例如256）\n",
    "- 将这个映射记录在查找表中\n",
    "- 查找表的大小是一个超参数，也就是“词表大小”（GPT-2为50,257）\n",
    "\n",
    "**3. 重复直至没有新收益**\n",
    "\n",
    "- 反复执行前两步，持续合并最常出现的组合\n",
    "- 当无法再次压缩时（例如不再有字组出现多于一次）即停\n",
    "\n",
    "**解压（解码）**\n",
    "\n",
    "- 要恢复原始文本，倒序执行替换，将每个ID用查找表中对应的字节对替之即可\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5ac9a-3528-4186-9468-8420c7b2ac00",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.4 BPE算法示例\n",
    "\n",
    "### 1.4.1 编码部分（步骤1和2）的实例\n",
    "\n",
    "- 假设有一段文本（训练数据集）`the cat in the hat`，想用它构建BPE分词器的词表\n",
    "\n",
    "**Iteration 1**\n",
    "\n",
    "1. 识别常见字符对\n",
    "  - 在这段文本中，\"th\"出现了两次（开头和第二个\"e\"之前）\n",
    "\n",
    "2. 替换并记录\n",
    "  - 用一个未被使用的新token ID代替\"th\"，比如256\n",
    "  - 新的文本：`<256>e cat in <256>e hat`\n",
    "  - 新的词表为\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "```\n",
    "\n",
    "**Iteration 2**\n",
    "\n",
    "1. **识别常见字符对**  \n",
    "   - 在`<256>e cat in <256>e hat`中，`<256>e`出现了两次\n",
    "\n",
    "2. **替换并记录**  \n",
    "   - 用一个新token ID代替`<256>e`，例如257  \n",
    "   - 新的文本为：\n",
    "     ```\n",
    "     <257> cat in <257> hat\n",
    "     ```\n",
    "   - 更新后的词表：\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     ```\n",
    "\n",
    "**Iteration 3**\n",
    "\n",
    "1. **识别常见字符对**  \n",
    "   - 在`<257> cat in <257> hat`中，`<257> `出现了两次（开头和\"hat\"前）\n",
    "\n",
    "2. **替换并记录**  \n",
    "   - 用一个新token ID代替`<257> `，例如258  \n",
    "   - 新的文本为：\n",
    "     ```\n",
    "     <258>cat in <258>hat\n",
    "     ```\n",
    "   - 更新后的词表：\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     258: \"<257> \"\n",
    "     ```\n",
    "     \n",
    "- 以此类推\n",
    "\n",
    "&nbsp;\n",
    "### 1.4.2 解码部分（步骤 3）的实例\n",
    "\n",
    "- 为了恢复原始文本，将每个token ID按引入顺序倒序替换回其对应的字符对\n",
    "- 以最终的压缩文本`<258>cat in <258>hat`为起点\n",
    "- 将`<258>`替换回`<257> `：`<257> cat in <257> hat`  \n",
    "- 将`<257>`替换回`<256>e`：`<256>e cat in <256>e hat`\n",
    "- 将`<256>`替换回\"th\"：`the cat in the hat`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2324948-ddd0-45d1-8ba8-e8eda9fc6677",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 2. 一个简单的 BPE 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ca709-40d7-4e3d-bf3e-4f5687a2e19b",
   "metadata": {},
   "source": [
    "- 下面的实现是一个Python类，模仿`tiktoken`的Python API\n",
    "- 需要注意，上面的编码部分描述的是通过`train()`进行的原始训练步骤；不过`encode()`方法的工作方式也很相似（只是因为处理特殊token，看起来稍微复杂一些）：\n",
    "\n",
    "1. 将输入文本拆分为单独字节\n",
    "2. 逐步寻找并替换（合并）相邻token（字节对），只要它们与已学习的BPE合并规则匹配（按照排名从高到低）\n",
    "3. 持续合并，直到不能再合并任何token\n",
    "4. 最终的token ID列表就是编码结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4a15ec-2667-4f56-b7c1-34e8071b621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess: Replace spaces with 'Ġ'\n",
    "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # Initialize vocab with unique characters, including 'Ġ' if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "\n",
    "        # Extend unique_chars with characters from processed_text that are not already included\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "\n",
    "        # Optionally, ensure 'Ġ' is included if it is relevant to your text processing\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "\n",
    "        # Now create the vocab and inverse vocab dictionaries\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:  # No more pairs to merge. Stopping training.\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to encode.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # Split text into tokens, keeping newlines intact\n",
    "        words = text.replace(\"\\n\", \" \\n \").split()  # Ensure '\\n' is treated as a separate token\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0 and not word.startswith(\"\\n\"):\n",
    "                tokens.append(\"Ġ\" + word)  # Add 'Ġ' to words that follow a space or newline\n",
    "            else:\n",
    "                tokens.append(word)  # Handle first word or standalone '\\n'\n",
    "\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                # token is contained in the vocabulary as is\n",
    "                token_id = self.inverse_vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "            else:\n",
    "                # Attempt to handle subword tokenization via BPE\n",
    "                sub_token_ids = self.tokenize_with_bpe(token)\n",
    "                token_ids.extend(sub_token_ids)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i], token_ids[i + 1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id = self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    # Uncomment for educational purposes:\n",
    "                    # print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                    i += 2  # Skip the next token as it's merged\n",
    "                    can_merge = True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i += 1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token.startswith(\"Ġ\"):\n",
    "                # Replace 'Ġ' with a space\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db7310-79c7-4ee0-b5fa-d760c6e1aa67",
   "metadata": {},
   "source": [
    "- `BPETokenizerSimple`类的代码较长，详细对每个细节的讨论不在本notebook范围之内，下一节会通过使用概览来帮助我们更好理解各个方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe1836-eed4-40dc-860b-2d23074d067e",
   "metadata": {},
   "source": [
    "## 3. BPE实现游览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c996c-fd34-484f-a877-13d977214cf7",
   "metadata": {},
   "source": [
    "- 实践上，强烈建议使用[tiktoken](https://github.com/openai/tiktoken)，因为上面的实现主要关注可读性和教学价值，而不是性能\n",
    "- 然而，其用法与tiktoken大致相同，只是tiktoken没有训练方法\n",
    "- 下面通过一些例子来看`BPETokenizerSimple` Python代码如何工作（探讨具体代码不在本notebook范围）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82acaf6-7ed5-4d3b-81c0-ae4d3559d2c7",
   "metadata": {},
   "source": [
    "### 3.1 训练、编码与解码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962bf037-903e-4555-b09c-206e1a410278",
   "metadata": {},
   "source": [
    "- 首先，我们以一段示例文本作为训练数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d197cad-ed10-4a42-b01c-a763859781fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"../01_main-chapter-code/the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"../01_main-chapter-code/the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open(\"../01_main-chapter-code/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f: # added ../01_main-chapter-code/\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1b6ac-71d3-4817-956a-9bc7e463a84a",
   "metadata": {},
   "source": [
    "- 接着，以1,000的词表大小初始化并训练BPE分词器\n",
    "- 注意由于前面提到的字节值，词表默认已经包含255个条目，因此只需要“学习”745个词表项\n",
    "- 作为对比，GPT-2的词表大小为50,257，GPT-4的词表为 00,256（tiktoken中的`cl100k_base`），而GPT-4o使用199,997（tiktoken中的`o200k_base`）；它们的训练数据都远大于上面的示例文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "027348fd-d52f-4396-93dd-38eed142df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(text, vocab_size=1000, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474ff05-5629-4f13-9e03-a47b1e713850",
   "metadata": {},
   "source": [
    "- 可能想查看词表的具体内容（但这会生成很长的列表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f705a283-355e-4460-b940-06bbc2ae4e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.vocab)\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9da0f-8a18-41cd-91ea-9ccc2bb5febb",
   "metadata": {},
   "source": [
    "- 该词表通过大约742次合并得到（约等于`1000 - len(range(0, 256))`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3da42d1c-f75c-4ba7-a6c5-4cb8543d4a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.bpe_merges))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac69c9-8413-482a-8148-6b2afbf1fb89",
   "metadata": {},
   "source": [
    "- 这意味着最前面的256个条目都是单字符token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a4108-7c8b-4b98-9c67-d622e9cdf250",
   "metadata": {},
   "source": [
    "- 接下来，使用`encode`方法基于这些合并规则对一些文本进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1db5cce-e015-412b-ad56-060b8b638078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ed1b344-f7d4-4e9e-ac34-2a04b5c5b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 42\n",
      "Number of token IDs: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1cfb9-402a-4e1e-9678-0b7547406248",
   "metadata": {},
   "source": [
    "- 从上述长度可以看到，一句42个字符的句子被编码成20个token ID，相比按字符/字节编码方式，输入长度几乎减半"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252693ee-e806-4dac-ab76-2c69086360f4",
   "metadata": {},
   "source": [
    "- 请注意，词表本身会在`decode()`方法中使用，以便将token ID映射回文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da0e1faf-1933-43d9-b681-916c282a8f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b690e83-5d6b-409a-804e-321c287c24a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea5d09-e5ef-4721-994b-b9b25662fa0a",
   "metadata": {},
   "source": [
    "- 逐个遍历每个token ID能帮助更好地理解词表是如何完成解码的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b9e6289-92cb-4d88-b3c8-e836d7c8095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 -> Jack\n",
      "256 ->  \n",
      "654 -> em\n",
      "531 -> br\n",
      "302 -> ac\n",
      "311 -> ed\n",
      "256 ->  \n",
      "296 -> be\n",
      "97 -> a\n",
      "465 -> ut\n",
      "121 -> y\n",
      "595 ->  through\n",
      "841 ->  ar\n",
      "116 -> t\n",
      "287 ->  a\n",
      "466 -> nd\n",
      "256 ->  \n",
      "326 -> li\n",
      "972 -> fe\n",
      "46 -> .\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea41c6c-5538-4fd5-8b5f-195960853b71",
   "metadata": {},
   "source": [
    "- 可以看到，大多数token ID表示2个字符的子词；这是因为训练文本非常短、重复词不多，并且选择了相对较小的词表大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600055a3-7ec8-4abf-b88a-c4186fb71463",
   "metadata": {},
   "source": [
    "- 总而言之，调用`decode(encode())`应该能够还原任意输入文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7056cb1-a9a3-4cf6-8364-29fb493ae240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"This is some text.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558af04-483c-4f6b-88f5-a534f37316cd",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 4. 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ed0e6-ad06-4bb3-bb39-6b8110c1caa4",
   "metadata": {},
   "source": [
    "- 就是这样！这就是BPE的整体工作流程，并包含一个可用于创建新分词器的训练方法 \n",
    "- 希望这个简短的教程在教学方面对你有所帮助；若有疑问，欢迎在[这里](https://github.com/rasbt/LLMs-from-scratch/discussions/categories/q-a)开启新的讨论\n",
    "\n",
    "**这是一个非常朴素的教学实现。[bpe-from-scratch.ipynb](bpe-from-scratch.ipynb)提供了一个更复杂（但也更难读）的实现，其行为与tiktoken一致。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
