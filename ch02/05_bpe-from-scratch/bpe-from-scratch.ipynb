{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从头实现BPE分词器\n",
    "- 这是一个独立的notebook，从零开始实现流行的字节对编码（BPE）分词算法，该算法用于 GPT-2 到 GPT-4、Llama 3 等模型\n",
    "- 关于分词目的的更多详情，请参考[第2章](https://github.com/zzfive/LLMs-from-scratch-bias/blob/main/ch02/01_main-chapter-code/ch02.ipynb)；这里的代码是解释 BPE 算法的额外材料\n",
    "- OpenAI 为训练原始 GPT 模型实现的原始 BPE 分词器可以在[这里](https://github.com/openai/gpt-2/blob/master/src/encoder.py)找到\n",
    "- BPE 算法最初是由 Philip Gage 在 1994 年的论文\"[A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\"中描述\n",
    "- 现在大多数项目，包括 Llama 3，都使用 OpenAI 的开源 [tiktoken 库](https://github.com/openai/tiktoken)，因为它的计算性能；例如，它允许加载预训练的 GPT-2 和 GPT-4 分词器（Llama 3 模型也是使用 GPT-4 分词器训练的）\n",
    "- 上述实现与本笔记本中的实现之间的区别，除了它之外，还包括一个用于训练分词器的函数（出于教育目的）\n",
    "- 还有一个名为 [minBPE](https://github.com/karpathy/minbpe) 的实现，它支持训练，可能更高效（这里的实现主要关注教育目的）；与 `minbpe` 相比，此处的实现还允许加载原始 OpenAI 分词器词汇表和 BPE \"合并\"（此外，Hugging Face 分词器也能够训练和加载各种分词器；更多信息请参见[这个 GitHub 讨论](https://github.com/rasbt/LLMs-from-scratch/discussions/485)，这是一位读者在尼泊尔语上训练 BPE 分词器的讨论）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 1 BPE背后的主要思想\n",
    "- BPE的主要想法是将文本转换为可用于LLM训练的整数表征，即token ID，如[Chapter 2](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)中所示\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/bpe-overview.webp\" width=\"600px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.1 Bits and bytes/位与字节\n",
    "- 在开始BPE之前，先引入字节的概念\n",
    "- 考虑将文本转换为字节数组（毕竟BPE代表\"字节\"对编码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text')\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "byte_array = bytearray(text, \"utf-8\")\n",
    "print(byte_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对bytearray对象调用list()，会每个字节被视为单独的元素，结果是一个与字节值对应的整数列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_array)  # 每一个字节对应一个字符，每个字节内存存储一个数值，唯一表示该字符\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以使用chr函数查看一个字节数值对应的字符\n",
    "chr(105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这是一种有效的方法，可以将文本转换为在大型语言模型(LLM)嵌入层中需要的token ID表示\n",
    "- 然而，这种方法的缺点是它为每个字符创建一个ID（对于短文本来说，这会产生很多ID）\n",
    "- 也就是说，对于一个17个字符的输入文本，必须使用17个标记ID作为LLM的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 17\n",
      "Number of token IDs: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果你之前使用过大型语言模型(LLM)，你可能知道BPE分词器有一个词汇表，为整个单词或子词分配token ID，而不是为每个字符分配token ID。\n",
    "- 例如，GPT-2分词器将相同的文本（\"This is some text\"）仅标记为4个标记而不是17个：1212, 318, 617, 2420\n",
    "- 可以使用交互式[tiktoken应用](https://tiktokenizer.vercel.app/?model=gpt2)或[tiktoken库](https://github.com/openai/tiktoken)来验证这一点：\n",
    "\n",
    "<center>\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/tiktokenizer.webp\" width=\"600px\">\n",
    "</center>\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")\n",
    "# prints [1212, 318, 617, 2420]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 因为一个字节包含8个bits，有2<sup>8</sup> = 256种可能的值，即一个字节可表示256不同的字符，从0~255\n",
    "- 执行以下代码 `bytearray(range(0, 257))`, 会收到异常 `ValueError: byte must be in range(0, 256)`)\n",
    "- BPE分词器通常使用这256个值作为其前256个**单字符标记**；可以通过运行以下代码进行直观检查：\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")\n",
    "\"\"\"\n",
    "prints:\n",
    "0: !\n",
    "1: \"\n",
    "2: #\n",
    "...\n",
    "255: �  # <---- single character tokens up to here\n",
    "256:  t\n",
    "257:  a\n",
    "...\n",
    "298: ent\n",
    "299:  n\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "- 上面需要注意的是，第256和257项不是单字符值，而是双字符值（一个空格+一个字母），这是原始GPT-2 BPE分词器的一个小缺点（在GPT-4分词器中已经得到改进）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 建立词汇表\n",
    "- BPE标记化算法的目标是构建一个包含常见子词的词汇表，如298: ent（例如可以在entangle, entertain, enter, entrance, entity, ...等词中找到），甚至是完整的单词\n",
    "\n",
    "```\n",
    "318: is\n",
    "617: some\n",
    "1212: This\n",
    "2420: text\n",
    "```\n",
    "\n",
    "- 在开始进行实际代码实现之前，如今用于LLM分词器的形式可以总结如下\n",
    "\n",
    "## 1.3 BPE算法概述\n",
    "\n",
    "**1. 识别频繁对**\n",
    "- 在每次迭代中，扫描文本以找到最常出现的字节对（或字符对）\n",
    "\n",
    "**2. 替换并记录**\n",
    "- 用一个新的占位符ID替换该对（使用尚未使用的ID，例如，如果我们从0...255开始，第一个占位符将是256）\n",
    "- 在查找表中记录这个映射\n",
    "- 查找表的大小是一个超参数，也称为\"词汇表大小\"（对于GPT-2，是50,257）\n",
    "\n",
    "**3. 重复直到没有收益**\n",
    "- 不断重复步骤1、2，持续合并最频繁的对\n",
    "- 当不能继续压缩后停止\n",
    "\n",
    "**解压缩/解码**\n",
    "- 恢复原始文本，使用查找表将每个ID替换为其对应的对，从而逆转该过程\n",
    "\n",
    "&nbsp;\n",
    "## 1.4 BPE算法例子\n",
    "\n",
    "### 1.4.1 上述步骤1、2的具体例子\n",
    "- 假设有文本(训练数据) \"the cat in the hat\"，要从中为BPE分词器构建词汇表\n",
    "**迭代1**\n",
    "1. 识别频繁对\n",
    "- 在这个文本中，\"th\"出现了两次\n",
    "2. 替换并记录\n",
    "- 用一个尚未使用的新标记ID替换\"th\"，例如，256\n",
    "- 新文本是：\"<256>e cat in <256>e hat\"\n",
    "- 新词汇表是\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "```\n",
    "\n",
    "**迭代2**\n",
    "1. 识别频繁对\n",
    "- 在文本<256>e cat in <256>e hat中，对<256>e出现了两次\n",
    "2. 替换并记录\n",
    "- 用一个尚未使用的新标记ID替换\"<256>e\"，例如，257\n",
    "- 新文本是：\"<257> cat in <257> hat\"\n",
    "- 新词汇表是\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "  257: \"<256>e\"\n",
    "```\n",
    "\n",
    "**迭代3**\n",
    "1. 识别频繁对\n",
    "- 在文本<257> cat in <257> hat中，对<257> 出现了两次\n",
    "2. 替换并记录\n",
    "- 用一个尚未使用的新标记ID替换\"<257> \"，例如，258\n",
    "- 新文本是：\"<258>cat in <258>hat\"\n",
    "- 新词汇表是\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "  257: \"<256>e\"\n",
    "  258: \"<257> \"\n",
    "```\n",
    "\n",
    "**依此类推**\n",
    "\n",
    "&nbsp;\n",
    "### 1.4.2 解码部分的具体例子(步骤3)\n",
    "- 要恢复原始文本，通过按照引入的相反顺序，用每个token ID对应的对来替换它们，从而逆转这个过程\n",
    "- 从最后的压缩文本开始：\"<258>cat in <258>hat\"\n",
    "- 代替 `<258>` → `<257> `: `<257> cat in <257> hat`  \n",
    "- 代替 `<257>` → `<256>e`: `<256>e cat in <256>e hat`\n",
    "- 代替 `<256>` → \"th\": `the cat in the hat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 一个简单的BPE实现\n",
    "- 下面是上述算法的Python类实现，模仿了tiktoken Python的用户接口\n",
    "- 请注意，上面的编码部分通过train()描述了原始训练步骤；然而，encode()方法的工作方式类似（尽管由于特殊标记处理看起来更复杂）:\n",
    "1. 将输入文本拆分为单个字节\n",
    "2. 重复查找并替换（合并）相邻标记（对），当它们匹配学习到的BPE合并中的任何对时（从最高到最低\"等级\"，即按照学习的顺序）\n",
    "3. 继续合并，直到无法应用更多合并\n",
    "4. 最终的标记ID列表就是编码输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from typing import Union, Tuple, List\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}  # 字典，key是元组，即合并的两个tokend_id，value是合并后的一个token_id\n",
    "\n",
    "    def train(self, text: str, vocab_size: int, allowed_special: set = {\"<|endoftext|>\"}) -> None:\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess: Replace spaces with 'Ġ'\n",
    "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):  # 将文本中的空格替换为Ġ\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # Initialize vocab with unique characters, including 'Ġ' if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "\n",
    "        # Extend unique_chars with characters from processed_text that are not already included\n",
    "        unique_chars.extend(\n",
    "            char for char in sorted(set(processed_text))\n",
    "            if char not in unique_chars\n",
    "        )\n",
    "\n",
    "        # Optionally, ensure 'Ġ' is included if it is relevant to your text processing\n",
    "        if \"Ġ\" not in unique_chars:\n",
    "            unique_chars.append(\"Ġ\")\n",
    "\n",
    "        # Now create the vocab and inverse vocab dictionaries\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:  # 添加允许的特殊token\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs；将processed_text以字符为单位转换为token_id\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):  # 从len(self.vocab)开始，直到设置的vocab_size\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")  # 从目前的token_ids中找到最频繁的pair\n",
    "            if pair_id is None:  # No more pairs to merge. Stopping training.\n",
    "                break\n",
    "            # 当前遍历的new_id就是上述找到的最频繁pair的新的token_id，用这个new_id替换token_ids中所有的pair_id\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id  # 记录pair_id合并过程，用于后续解码\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained vocabulary and BPE merges from OpenAI's GPT-2 files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json').\n",
    "            bpe_merges_path (str): Path to the bpe_merges file  (GPT-2 calls it 'vocab.bpe').\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # Convert loaded vocabulary to correct format\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Handle newline character without adding a new token 处理换行符\n",
    "        if \"\\n\" not in self.inverse_vocab:\n",
    "            # Use an existing token ID as a placeholder for '\\n'\n",
    "            # Preferentially use \"<|endoftext|>\" if available\n",
    "            # 尝试使用现有的特殊 token 作为换行符的占位符，优先使用\"<|endoftext|>\"，如果不可用，则使用\"Ġ\"，如果也不可用，则使用\"\"\n",
    "            fallback_token = next((token for token in [\"<|endoftext|>\", \"Ġ\", \"\"] if token in self.inverse_vocab), None)\n",
    "            if fallback_token is not None:\n",
    "                newline_token_id = self.inverse_vocab[fallback_token]\n",
    "            else:\n",
    "                # If no fallback token is available, raise an error\n",
    "                raise KeyError(\"No suitable token found in vocabulary to map '\\\\n'.\")\n",
    "\n",
    "            self.inverse_vocab[\"\\n\"] = newline_token_id\n",
    "            self.vocab[newline_token_id] = \"\\n\"\n",
    "\n",
    "        # Load BPE merges  加载BPE合并规则\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            # Skip header line if present\n",
    "            if lines and lines[0].startswith(\"#\"):  # 跳过可能存在的注释行\n",
    "                lines = lines[1:]\n",
    "\n",
    "            for line in lines:\n",
    "                pair = tuple(line.strip().split())  # 将每行按空格分割成一个元组    \n",
    "                if len(pair) == 2:  # 确保每行只有两个元素\n",
    "                    token1, token2 = pair  # 将元组中的两个元素分别赋值给token1和token2\n",
    "                    if token1 in self.inverse_vocab and token2 in self.inverse_vocab:  # 确保token1和token2都在词汇表中\n",
    "                        token_id1 = self.inverse_vocab[token1]\n",
    "                        token_id2 = self.inverse_vocab[token2]\n",
    "                        merged_token = token1 + token2\n",
    "                        if merged_token in self.inverse_vocab:  # 确保合并后的token也在词汇表中\n",
    "                            merged_token_id = self.inverse_vocab[merged_token]\n",
    "                            self.bpe_merges[(token_id1, token_id2)] = merged_token_id\n",
    "                        # print(f\"Loaded merge: '{token1}' + '{token2}' -> '{merged_token}' (ID: {merged_token_id})\")\n",
    "                        else:\n",
    "                            print(f\"Merged token '{merged_token}' not found in vocab. Skipping.\")\n",
    "                    else:\n",
    "                        print(f\"Skipping pair {pair} as one of the tokens is not in the vocabulary.\")\n",
    "\n",
    "    def encode(self, text: str, allowed_special: set = None) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs.\n",
    "        \"\"\"\n",
    "        import re\n",
    "\n",
    "        token_ids = []\n",
    "    \n",
    "        # If special token handling is enabled\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            # Build regex to match allowed special tokens\n",
    "            special_pattern = (\n",
    "                \"(\" + \"|\".join(re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)) + \")\"\n",
    "            )\n",
    "    \n",
    "            last_index = 0\n",
    "            for match in re.finditer(special_pattern, text):\n",
    "                prefix = text[last_index:match.start()]\n",
    "                token_ids.extend(self.encode(prefix, allowed_special=None))  # Encode prefix without special handling\n",
    "    \n",
    "                special_token = match.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f\"Special token {special_token} not found in vocabulary.\")\n",
    "                last_index = match.end()\n",
    "    \n",
    "            text = text[last_index:]  # Remaining part to process normally\n",
    "    \n",
    "            # Check if any disallowed special tokens are in the remainder\n",
    "            disallowed = [\n",
    "                tok for tok in self.inverse_vocab\n",
    "                if tok.startswith(\"<|\") and tok.endswith(\"|>\") and tok in text and tok not in allowed_special\n",
    "            ]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "        \n",
    "        # If no special tokens, or remaining text after special token split:\n",
    "        tokens = []\n",
    "        # First split on newlines to preserve them\n",
    "        lines = text.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            if i > 0:\n",
    "                tokens.append(\"\\n\")\n",
    "            words = line.split()\n",
    "            for j, word in enumerate(words):\n",
    "                if j == 0 and i > 0:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "                elif j == 0:\n",
    "                    tokens.append(word)\n",
    "                else:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:  # 如果token在词汇表中，则直接添加到token_ids中\n",
    "                # token is contained in the vocabulary as is\n",
    "                token_ids.append(self.inverse_vocab[token])\n",
    "            else:  # 如果token不在词汇表中，则需要通过BPE进行处理\n",
    "                # Attempt to handle subword tokenization via BPE\n",
    "                sub_token_ids = self.tokenize_with_bpe(token)  # 将token进行BPE处理\n",
    "                token_ids.extend(sub_token_ids)  # 将处理后的token_ids添加到token_ids中\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:  # 如果token_ids中存在None，则说明token中存在词汇表中不存在的字符\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]  # 找出token中哪些字符不在词汇表中\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")  # 抛出异常，提示哪些字符不在词汇表中\n",
    "\n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:  # 当can_merge为False或token_ids长度小于1时表明不可合并，不会执行\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i], token_ids[i + 1])  # 获取当前token_ids中的相邻两个元素\n",
    "                if pair in self.bpe_merges:  # 如果pair在BPE合并规则中\n",
    "                    merged_token_id = self.bpe_merges[pair]  # 获取合并后的token_id\n",
    "                    new_tokens.append(merged_token_id)  # 将合并后的token_id添加到new_tokens中\n",
    "                    # Uncomment for educational purposes:\n",
    "                    # print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                    i += 2  # Skip the next token as it's merged\n",
    "                    can_merge = True\n",
    "                else:  # 如果pair不在BPE合并规则中，则将当前token添加到new_tokens中\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i += 1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens  # 更新token_ids，继续下一轮合并\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token == \"\\n\":\n",
    "                if decoded_string and not decoded_string.endswith(\" \"):\n",
    "                    decoded_string += \" \"  # Add space if not present before a newline\n",
    "                decoded_string += token\n",
    "            elif token.startswith(\"Ġ\"):\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Save the vocabulary and BPE merges to JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to save the vocabulary.\n",
    "            bpe_merges_path (str): Path to save the BPE merges.\n",
    "        \"\"\"\n",
    "        # Save vocabulary\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(self.vocab, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Save BPE merges as a list of dictionaries\n",
    "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load the vocabulary and BPE merges from JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocabulary file.\n",
    "            bpe_merges_path (str): Path to the BPE merges file.\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Load BPE merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge[\"pair\"])\n",
    "                new_id = merge[\"new_id\"]\n",
    "                self.bpe_merges[pair] = new_id\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids: List[int], mode: str = \"most\") -> Union[Tuple[int, int], None]:  # 基于mode可以找到最频繁或最不频繁的pair\n",
    "        # zip(token_ids, token_ids[1:])先创建一个迭代器，生成相邻标记对的元组；假设token_ids = [1, 2, 3, 4]，会生成 [(1, 2), (2, 3), (3, 4)]\n",
    "        # Counter对上述生成的迭代器计数，就是计算每个元素出现的次数，返回一个字典，key为标记对，value为出现的次数\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if not pairs:\n",
    "            return None\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]  # 返回出现次数最多的pair\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]  # 返回出现次数最少的pair\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids: List[int], pair_id: Tuple[int, int], new_id: int) -> List[int]:\n",
    "        dq = deque(token_ids)  # 便于高效地从左侧移除元素\n",
    "        replaced = []  # 记录替换后的token_ids\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()  # 先从左侧取出第一个元素\n",
    "            if dq and (current, dq[0]) == pair_id:  # 如果当前元素和下一个元素组成的pair和传入的pair_id相同\n",
    "                replaced.append(new_id)  # 将new_id添加到replaced中\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()  # 因为已经将dq中第一个元素和current合并，所以也要将其移除\n",
    "            else:\n",
    "                replaced.append(current)  # 如果当前元素和下一个元素组成的pair和传入的pair_id不同，则将当前元素添加到replaced中\n",
    "\n",
    "        return replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BPETokenizerSimple类中有很多代码，详细讨论它超出了本笔记本的范围，但下一部分提供了一个简短的使用概述，以便更好地理解类方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 BPE实现演练\n",
    "- 在实践中，强烈推荐使用[tiktoken](https://github.com/openai/tiktoken)，因为上面的实现主要关注可读性和教育目的，而不是性能\n",
    "- 然而，使用方法与tiktoken大致相似，只是tiktoken没有训练方法\n",
    "- 通过下面的一些例子来看看上面的`BPETokenizerSimple` Python代码是如何工作的（详细的代码讨论超出了本笔记本的范围）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 训练、编码和解码\n",
    "- 首先，使用一些文本作为训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"../01_main-chapter-code/the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"../01_main-chapter-code/the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open(\"../01_main-chapter-code/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f: # added ../01_main-chapter-code/\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 接下来，初始化并训练BPE分词器，词汇表大小为1,000\n",
    "- 请注意，由于之前讨论的字节值，词汇表大小默认已经是256，所以只\"学习\"744个词汇条目（如果考虑<|endoftext|>特殊标记和Ġ空白标记；所以，确切地说是742个）\n",
    "- 作为比较，GPT-2词汇表有50,257个标记，GPT-4词汇表有100,256个标记（tiktoken中的`cl100k_base`），而GPT-4o使用199,997个标记（tiktoken中的`o200k_base`）；与上面的简单示例文本相比，它们都有更大的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(text, vocab_size=1000, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可能想检查词汇表内容（但请注意，这会创建一个很长的列表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.vocab)\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这个词汇表是通过合并742次创建的（= 1000 - len(range(0, 256)) - len(special_tokens) - \"Ġ\" = 1000 - 256 - 1 - 1 = 742）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.bpe_merges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这意味前256个单一token\n",
    "- 接下来使用学习的tokenizer编码一些文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46, 60, 124, 271, 683, 102, 116, 461, 116, 124, 62]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.<|endoftext|> \"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46, 257]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.<|endoftext|> \"\n",
    "token_ids = tokenizer.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 56\n",
      "Number of token IDs: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从上面的长度可以看出，一个42个字符的句子被编码成20个标记ID，与基于字符字节的编码相比，有效地将输入长度大约减少了一半。\n",
    "- 请注意，词汇表本身在decode()方法中使用，它允许将标记ID映射回文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46, 257]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过遍历每个标记ID，可以更好地理解标记ID是如何通过词汇表解码的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 -> Jack\n",
      "256 ->  \n",
      "654 -> em\n",
      "531 -> br\n",
      "302 -> ac\n",
      "311 -> ed\n",
      "256 ->  \n",
      "296 -> be\n",
      "97 -> a\n",
      "465 -> ut\n",
      "121 -> y\n",
      "595 ->  through\n",
      "841 ->  ar\n",
      "116 -> t\n",
      "287 ->  a\n",
      "466 -> nd\n",
      "256 ->  \n",
      "326 -> li\n",
      "972 -> fe\n",
      "46 -> .\n",
      "257 -> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 正如上所示，大多数token IDs代表2个字符的子词；这是因为训练数据文本非常短，没有那么多重复的单词，而且使用了相对较小的词汇表大小\n",
    "- 总结一下，调用decode(encode())应该能够重现任意输入文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    tokenizer.encode(\"This is some text.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text with \\n newline characters.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    tokenizer.encode(\"This is some text with \\n newline characters.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 保存和加载分词器\n",
    "- 解析来为如何保存和重复使用分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存分词器\n",
    "tokenizer.save_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载分词器\n",
    "tokenizer2 = BPETokenizerSimple()\n",
    "tokenizer2.load_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加载的分词器应该可以与之前的产生一样的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text with \\n newline characters.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(\n",
    "    tokenizer.encode(\"This is some text with \\n newline characters.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 加载原始的GPT-2分词器\n",
    "- 最后，加载原始的GPT-2分词器种的词汇表和合并文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab.bpe already exists in ../02_bonus_bytepair-encoder/gpt2_model/vocab.bpe\n",
      "encoder.json already exists in ../02_bonus_bytepair-encoder/gpt2_model/encoder.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def download_file_if_absent(url, filename, search_dirs):\n",
    "    for directory in search_dirs:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"{filename} already exists in {file_path}\")\n",
    "            return file_path\n",
    "\n",
    "    target_path = os.path.join(search_dirs[0], filename)\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response, open(target_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "        print(f\"Downloaded {filename} to {target_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filename}. Error: {e}\")\n",
    "    return target_path\n",
    "\n",
    "# Define the directories to search and the files to download\n",
    "search_directories = [\".\", \"../02_bonus_bytepair-encoder/gpt2_model/\"]\n",
    "\n",
    "files_to_download = {\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe\": \"vocab.bpe\",\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json\": \"encoder.json\"\n",
    "}\n",
    "\n",
    "# Ensure directories exist and download files if needed\n",
    "paths = {}\n",
    "for url, filename in files_to_download.items():\n",
    "    paths[filename] = download_file_if_absent(url, filename, search_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用load_vocab_and_merges_from_openai方法加载GPT-2分词器的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2 = BPETokenizerSimple()\n",
    "tokenizer_gpt2.load_vocab_and_merges_from_openai(\n",
    "    vocab_path=paths[\"encoder.json\"], bpe_merges_path=paths[\"vocab.bpe\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用以下命令查看词汇表大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_gpt2.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 显示能通过BPETokenizerSimple类使用GPT-2分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 617, 2420]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"This is some text\"\n",
    "token_ids = tokenizer_gpt2.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is some text\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_gpt2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以使用交互式[tiktoken应用](https://tiktokenizer.vercel.app/?model=gpt2)或[tiktoken库](https://github.com/openai/tiktoken)来验证这是否产生了正确的标记：\n",
    "\n",
    "<center>\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/tiktokenizer.webp\" width=\"600px\">\n",
    "</center>\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")\n",
    "# prints [1212, 318, 617, 2420]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 结论\n",
    "- 就是这样！这就是BPE的工作原理，包括用于创建新分词器的训练方法，或从原始OpenAI GPT-2模型加载GPT-2分词器词汇表和合并规则\n",
    "- 希望你发现这个简短的教程对教育目的有用；如果你有任何问题，请随时在[这里](https://github.com/rasbt/LLMs-from-scratch/discussions/categories/q-a)开启新的讨论\n",
    "- 关于与其他分词器实现的性能比较，请参阅[这个笔记本](https://github.com/zzfive/LLMs-from-scratch-bias/blob/main/ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
