{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从头实现BPE分词器\n",
    "- 这是一个独立的notebook，从零开始实现流行的字节对编码（BPE）分词算法，该算法用于 GPT-2 到 GPT-4、Llama 3 等模型\n",
    "- 关于分词目的的更多详情，请参考[第2章](https://github.com/zzfive/LLMs-from-scratch-bias/blob/main/ch02/01_main-chapter-code/ch02.ipynb)；这里的代码是解释 BPE 算法的额外材料\n",
    "- OpenAI 为训练原始 GPT 模型实现的原始 BPE 分词器可以在[这里](https://github.com/openai/gpt-2/blob/master/src/encoder.py)找到\n",
    "- BPE 算法最初是由 Philip Gage 在 1994 年的论文\"[A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\"中描述\n",
    "- 现在大多数项目，包括 Llama 3，都使用 OpenAI 的开源 [tiktoken 库](https://github.com/openai/tiktoken)，因为它的计算性能；例如，它允许加载预训练的 GPT-2 和 GPT-4 分词器（Llama 3 模型也是使用 GPT-4 分词器训练的）\n",
    "- 上述实现与本笔记本中的实现之间的区别，除了它之外，还包括一个用于训练分词器的函数（出于教育目的）\n",
    "- 还有一个名为 [minBPE](https://github.com/karpathy/minbpe) 的实现，它支持训练，可能更高效（这里的实现主要关注教育目的）；与 `minbpe` 相比，此处的实现还允许加载原始 OpenAI 分词器词汇表和 BPE \"合并\"（此外，Hugging Face 分词器也能够训练和加载各种分词器；更多信息请参见[这个 GitHub 讨论](https://github.com/rasbt/LLMs-from-scratch/discussions/485)，这是一位读者在尼泊尔语上训练 BPE 分词器的讨论）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# BPE背后的主要思想\n",
    "- BPE的主要想法是将文本转换为可用于LLM训练的整数表征，即token ID，如[Chapter 2](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)中所示\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/bpe-overview.webp\" width=\"600px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.1 Bits and bytes/位与字节\n",
    "- 在开始BPE之前，先引入字节的概念\n",
    "- 考虑将文本转换为字节数组（毕竟BPE代表\"字节\"对编码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text')\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "byte_array = bytearray(text, \"utf-8\")\n",
    "print(byte_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对bytearray对象调用list()，会每个字节被视为单独的元素，结果是一个与字节值对应的整数列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_array)  # 每一个字节对应一个字符，每个字节内存存储一个数值，唯一表示该字符\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以使用chr函数查看一个字节数值对应的字符\n",
    "chr(105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这是一种有效的方法，可以将文本转换为在大型语言模型(LLM)嵌入层中需要的token ID表示\n",
    "- 然而，这种方法的缺点是它为每个字符创建一个ID（对于短文本来说，这会产生很多ID）\n",
    "- 也就是说，对于一个17个字符的输入文本，必须使用17个标记ID作为LLM的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 17\n",
      "Number of token IDs: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果你之前使用过大型语言模型(LLM)，你可能知道BPE分词器有一个词汇表，为整个单词或子词分配token ID，而不是为每个字符分配token ID。\n",
    "- 例如，GPT-2分词器将相同的文本（\"This is some text\"）仅标记为4个标记而不是17个：1212, 318, 617, 2420\n",
    "- 可以使用交互式[tiktoken应用](https://tiktokenizer.vercel.app/?model=gpt2)或[tiktoken库](https://github.com/openai/tiktoken)来验证这一点：\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/tiktokenizer.webp\" width=\"600px\">\n",
    "</p>\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")\n",
    "# prints [1212, 318, 617, 2420]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 因为一个字节包含8个bits，有2<sup>8</sup> = 256种可能的值，即一个字节可表示256不同的字符，从0~255\n",
    "- 执行以下代码 `bytearray(range(0, 257))`, 会收到异常 `ValueError: byte must be in range(0, 256)`)\n",
    "- BPE分词器通常使用这256个值作为其前256个**单字符标记**；可以通过运行以下代码进行直观检查：\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")\n",
    "\"\"\"\n",
    "prints:\n",
    "0: !\n",
    "1: \"\n",
    "2: #\n",
    "...\n",
    "255: �  # <---- single character tokens up to here\n",
    "256:  t\n",
    "257:  a\n",
    "...\n",
    "298: ent\n",
    "299:  n\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "- 上面需要注意的是，第256和257项不是单字符值，而是双字符值（一个空格+一个字母），这是原始GPT-2 BPE分词器的一个小缺点（在GPT-4分词器中已经得到改进）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 建立词汇表\n",
    "- BPE标记化算法的目标是构建一个包含常见子词的词汇表，如298: ent（例如可以在entangle, entertain, enter, entrance, entity, ...等词中找到），甚至是完整的单词\n",
    "\n",
    "```\n",
    "318: is\n",
    "617: some\n",
    "1212: This\n",
    "2420: text\n",
    "```\n",
    "\n",
    "- 在开始进行实际代码实现之前，如今用于LLM分词器的形式可以总结如下\n",
    "\n",
    "## 1.3 BPE算法概述\n",
    "\n",
    "**1. 识别频繁对**\n",
    "- 在每次迭代中，扫描文本以找到最常出现的字节对（或字符对）\n",
    "\n",
    "**2. 替换并记录**\n",
    "- 用一个新的占位符ID替换该对（使用尚未使用的ID，例如，如果我们从0...255开始，第一个占位符将是256）\n",
    "- 在查找表中记录这个映射\n",
    "- 查找表的大小是一个超参数，也称为\"词汇表大小\"（对于GPT-2，是50,257）\n",
    "\n",
    "**3. 重复直到没有收益**\n",
    "- 不断重复步骤1、2，持续合并最频繁的对\n",
    "- 当不能继续压缩后停止\n",
    "\n",
    "**解压缩/解码**\n",
    "- 恢复原始文本，使用查找表将每个ID替换为其对应的对，从而逆转该过程\n",
    "\n",
    "&nbsp;\n",
    "## 1.4 BPE算法例子\n",
    "\n",
    "### 1.4.1 上述步骤1、2的具体例子\n",
    "- 假设有文本(训练数据) \"the cat in the hat\"，要从中为BPE分词器构建词汇表\n",
    "**迭代1**\n",
    "1. 识别频繁对\n",
    "- 在这个文本中，\"th\"出现了两次\n",
    "2. 替换并记录\n",
    "- 用一个尚未使用的新标记ID替换\"th\"，例如，256\n",
    "- 新文本是：\"<256>e cat in <256>e hat\"\n",
    "- 新词汇表是\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "```\n",
    "\n",
    "**迭代2**\n",
    "1. 识别频繁对\n",
    "- 在文本<256>e cat in <256>e hat中，对<256>e出现了两次\n",
    "2. 替换并记录\n",
    "- 用一个尚未使用的新标记ID替换\"<256>e\"，例如，257\n",
    "- 新文本是：\"<257> cat in <257> hat\"\n",
    "- 新词汇表是\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "  257: \"<256>e\"\n",
    "```\n",
    "\n",
    "**迭代3**\n",
    "1. 识别频繁对\n",
    "- 在文本<257> cat in <257> hat中，对<257> 出现了两次\n",
    "2. 替换并记录\n",
    "- 用一个尚未使用的新标记ID替换\"<257> \"，例如，258\n",
    "- 新文本是：\"<258>cat in <258>hat\"\n",
    "- 新词汇表是\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "  257: \"<256>e\"\n",
    "  258: \"<257> \"\n",
    "```\n",
    "\n",
    "**依此类推**\n",
    "\n",
    "&nbsp;\n",
    "### 1.4.2 解码部分的具体例子(步骤3)\n",
    "- 要恢复原始文本，通过按照引入的相反顺序，用每个token ID对应的对来替换它们，从而逆转这个过程\n",
    "- 从最后的压缩文本开始：\"<258>cat in <258>hat\"\n",
    "- 代替 `<258>` → `<257> `: `<257> cat in <257> hat`  \n",
    "- 代替 `<257>` → `<256>e`: `<256>e cat in <256>e hat`\n",
    "- 代替 `<256>` → \"th\": `the cat in the hat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 一个简单的BPE实现\n",
    "- 下面是上述算法的Python类实现，模仿了tiktoken Python的用户接口\n",
    "- 请注意，上面的编码部分通过train()描述了原始训练步骤；然而，encode()方法的工作方式类似（尽管由于特殊标记处理看起来更复杂）:\n",
    "1. 将输入文本拆分为单个字节\n",
    "2. 重复查找并替换（合并）相邻标记（对），当它们匹配学习到的BPE合并中的任何对时（从最高到最低\"等级\"，即按照学习的顺序）\n",
    "3. 继续合并，直到无法应用更多合并\n",
    "4. 最终的标记ID列表就是编码输出"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
